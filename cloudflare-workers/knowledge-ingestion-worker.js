/**
 * Knowledge Ingestion Worker
 * Processes pending knowledge_ingestion_jobs and writes chunks + embeddings.
 * This is a lightweight pipeline to prove ingestion works end-to-end.
 */

function generateId() {
  return crypto.randomUUID()
}

function embedText(text) {
  const tokens = (text || '').toLowerCase().split(/\s+/).filter(Boolean)
  const vec = Array(8).fill(0)
  tokens.forEach((tok, idx) => {
    const code = Array.from(tok).reduce((sum, ch) => sum + ch.charCodeAt(0), 0)
    vec[idx % vec.length] += code
  })
  const norm = Math.sqrt(vec.reduce((sum, v) => sum + v * v, 0)) || 1
  return vec.map(v => v / norm)
}

async function claimJobs(db, limit = 5) {
  const rows = await db.prepare(`
    SELECT * FROM knowledge_ingestion_jobs
    WHERE status = 'pending'
    ORDER BY datetime(created_at) ASC
    LIMIT ?
  `).bind(limit).all()

  const claimed = []
  for (const row of rows.results || []) {
    const updated = await db.prepare(`
      UPDATE knowledge_ingestion_jobs
      SET status = 'processing', updated_at = datetime('now')
      WHERE id = ? AND status = 'pending'
    `).bind(row.id).run()

    if (updated.success !== false) {
      claimed.push(row)
    }
  }
  return claimed
}

async function processJob(db, job) {
  const now = new Date().toISOString()

  const doc = await db.prepare('SELECT * FROM knowledge_documents WHERE id = ?').bind(job.document_id).first()
  if (!doc) {
    throw new Error('Document missing for ingestion job')
  }

  // Determine next version number
  const latest = await db.prepare('SELECT MAX(version_number) as max_version FROM knowledge_versions WHERE document_id = ?').bind(doc.id).first()
  const nextVersion = (latest?.max_version || 0) + 1
  const versionId = generateId()

  await db.prepare(`
    INSERT INTO knowledge_versions (id, document_id, org_id, version_number, checksum, chunk_count, token_count, created_at, created_by, change_note, stage, is_live)
    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'live', 1)
  `).bind(
    versionId,
    doc.id,
    doc.org_id,
    nextVersion,
    doc.checksum || `checksum_${versionId}`,
    1,
    0,
    now,
    'ingestion_worker',
    'Auto-ingested',
  ).run()

  // Build a simple chunk from the document metadata; in a real pipeline this would split file contents.
  const chunkId = generateId()
  const content = doc.path || doc.title || 'Untitled document'
  const tokenCount = Math.max(10, Math.round(content.length / 4))
  await db.prepare(`
    INSERT INTO knowledge_chunks (id, version_id, org_id, content, token_count, page_label, metadata)
    VALUES (?, ?, ?, ?, ?, ?, ?)
  `).bind(
    chunkId,
    versionId,
    doc.org_id,
    content,
    tokenCount,
    'auto',
    JSON.stringify({ autogenerated: true })
  ).run()

  // Write an embedding using deterministic hash-based vector
  const vector = embedText(content)
  await db.prepare(`
    INSERT INTO knowledge_embeddings (id, org_id, version_id, chunk_id, model, vector)
    VALUES (?, ?, ?, ?, ?, ?)
  `).bind(
    generateId(),
    doc.org_id,
    versionId,
    chunkId,
    'hashed-embed-v1',
    JSON.stringify(vector)
  ).run()

  // Mark document live version
  await db.prepare('UPDATE knowledge_documents SET live_version_id = ?, updated_at = datetime(\'now\') WHERE id = ?').bind(versionId, doc.id).run()

  // Mark job success
  await db.prepare('UPDATE knowledge_ingestion_jobs SET status = ?, version_id = ?, updated_at = datetime(\'now\'), error_message = NULL WHERE id = ?').bind('succeeded', versionId, job.id).run()
}

async function runIngestion(db) {
  const jobs = await claimJobs(db)
  for (const job of jobs) {
    try {
      await processJob(db, job)
    } catch (err) {
      await db.prepare('UPDATE knowledge_ingestion_jobs SET status = ?, error_message = ?, updated_at = datetime(\'now\') WHERE id = ?').bind('failed', err.message, job.id).run()
    }
  }
  return jobs.length
}

export default {
  async fetch(request, env) {
    if (request.method !== 'POST') {
      return new Response('Method Not Allowed', { status: 405 })
    }

    const processed = await runIngestion(env.DB)
    return new Response(JSON.stringify({ processed }), {
      status: 200,
      headers: { 'content-type': 'application/json' },
    })
  },
}
